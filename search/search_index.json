{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#welcome-to-celai","title":"Welcome to Cel.ai","text":"<p>Cel.ai is a powerful Python framework designed to accelerate the development of omnichannel virtual assistants. Whether you need to integrate with platforms like WhatsApp, Telegram, or VoIP services such as VAPI.com, Cel.ai provides the tools and flexibility to get your assistant up and running quickly.</p>"},{"location":"#why-celai","title":"Why Cel.ai?","text":"<ul> <li>Rapid Development: Quickly deploy AI assistants across multiple messaging platforms.</li> <li>Flexibility: Easily create custom connectors, middlewares and assistants.</li> <li>Extensibility: Use middlewares to add custom functionality, decode messages, handle security, and session management.</li> </ul> <p>Explore the documentation to learn more about how Cel.ai can help you build powerful, omnichannel virtual assistants with ease. Feel free to customize this introduction to better fit your specific needs and use cases.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Design: Designed with a modular architecture that allows you to easily extend and customize its functionality.</li> <li>Asynchronous: Built on top of the FastAPI framework, Cel.ai is designed to be asynchronous and scalable.</li> <li>Streaming: Supports different stream modes to optimize the user experience based on the specific requirements of the platform and the nature of the interaction. </li> <li>Runs on Script: This system is designed with the principle that it can run a minimal QA/FAQ + RAG assistant in a single script without requiring databases, external services, or complex configurations.</li> <li>Open Source: Cel.ai is open source and actively maintained by the community.</li> </ul>"},{"location":"#overview","title":"Overview","text":""},{"location":"#key-components","title":"Key Components","text":""},{"location":"#connectors","title":"Connectors","text":"<p>Cel.ai comes with out-of-the-box support for several popular platforms:</p> <ul> <li>WhatsApp</li> <li>Telegram</li> <li>VAPI.com</li> <li>CLI</li> </ul> <p>These connectors translate messages between the platform and Cel.ai's agnostic message format, allowing the assistant to interact with different messaging platforms seamlessly.  Additionally, the framework allows for the creation of custom connectors, enabling seamless integration with virtually any platform.</p>"},{"location":"#middlewares","title":"Middlewares","text":"<p>Cel.ai supports the use of middlewares to extend and customize the functionality of your virtual assistants. This allows for greater flexibility and control over how your assistant processes and responds to user interactions. Middlewares can be used to add custom logic, perform data validation, or perform authentication, rate limiting, blacklisting, and more.</p> <p>You will find useful middlewares for decoding messages into a suitable format for Natural Language Processing (NLP) models. Take a look of <code>GeodecodingMiddleware</code> which decodes a message shared location into a human-readable address using the Google Maps API. </p> <p>So you can easily create your own middleware to add custom functionality to your assistant. For example, you can create a <code>PDFDecodingMiddleware</code> to decode a PDF file shared by the user into a text format that can be processed by the assistant.</p>"},{"location":"#assistants","title":"Assistants","text":"<p>At the core of Cel.ai are Assistants, which handle everything from conversation history persistence to state management and Retrieval-Augmented Generation (RAG). The framework includes a built-in assistant named Macaw, which is implemented using LangChain. However, you can also create your own assistant using the framework or any LLM model of your choice.</p>"},{"location":"#message-enhancers","title":"Message Enhancers","text":"<p>Message Enhancers are used to process the assistant's responses before they are sent to the user. This can include formatting the response, adding additional information, or enhancing the response with additional data. For example, you can use a <code>SmartMessageEnhancerOpenAI</code> to enhance the assistant's responses using the OpenAI API.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Welcome to the Cel.ai documentation! This guide will help you get started with creating your first omnichannel virtual assistant using the Cel.ai framework.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.11 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>To install Cel.ai, you can use pip. Run the following command in your terminal:</p> <pre><code>pip install celai\n</code></pre>"},{"location":"getting_started/#creating-your-first-assistant","title":"Creating Your First Assistant","text":"<p>Follow these steps to create your first virtual assistant with Cel.ai.</p>"},{"location":"getting_started/#step-1-initialize-your-project","title":"Step 1: Initialize Your Project","text":"<p>Create a new directory for your project and navigate into it:</p> <pre><code>mkdir my_celai_assistant\ncd my_celai_assistant\n</code></pre>"},{"location":"getting_started/#step-2-create-a-virtual-environment","title":"Step 2: Create a Virtual Environment","text":"<p>It's a good practice to use a virtual environment for your project. Run the following commands to create and activate a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n</code></pre>"},{"location":"getting_started/#step-3-install-celai","title":"Step 3: Install Cel.ai","text":"<p>With your virtual environment activated, install Cel.ai:</p> <pre><code>pip install celai\n</code></pre>"},{"location":"getting_started/#step-4-create-your-assistant-script","title":"Step 4: Create Your Assistant Script","text":"<p>If you are using Macaw Assistant, you need to add a environment variable <code>OPENAI_API_KEY</code> with your OpenAI API key.</p> <pre><code>export OPENAI_API_KEY=\"your_openai_api_key\"\n</code></pre> <p>For simplicity, we will create a basic assistant that uses the CLI connector to interact with the assistant via the command line interface. Create a new Python file, <code>assistant.py</code>, and open it in your favorite text editor. Add the following code to set up a basic assistant:</p> main.py<pre><code># Import Cel.ai modules\nfrom cel.connectors.cli.cli_connector import CliConnector\nfrom cel.gateway.message_gateway import MessageGateway, StreamMode\nfrom cel.message_enhancers.smart_message_enhancer_openai import SmartMessageEnhancerOpenAI\nfrom cel.assistants.macaw.macaw_assistant import MacawAssistant\nfrom cel.prompt.prompt_template import PromptTemplate\n\n# Setup prompt\nprompt = \"\"\"You are an AI assistant. Called Celia. You can help a user to buy Bitcoins.\"\"\"\nprompt_template = PromptTemplate(prompt)\n\n# Create the assistant based on the Macaw Assistant \nast = MacawAssistant(\n    prompt=prompt_template\n)\n\n# Create the Message Gateway - This component is the core of the assistant\n# It handles the communication between the assistant and the connectors\ngateway = MessageGateway(ast)\n\n# For this example, we will use the Telegram connector\nconn = CliConnector(\n    stream_mode=StreamMode.FULL\n)\n# Register the connector with the gateway\ngateway.register_connector(conn)\n\n# Then start the gateway and begin processing messages\ngateway.run()\n</code></pre>"},{"location":"getting_started/#step-5-run-your-assistant","title":"Step 5: Run Your Assistant","text":"<p>Save your <code>main.py</code> file and run it using the following command:</p> <pre><code>python main.py\n</code></pre> <p>Then you can interact with your assistant through the command line interface. Just type your message and press Enter to send it to the assistant.</p>"},{"location":"connectors/","title":"Introduction","text":"<p>Welcome to the Cel.ai Connectors documentation. This page provides an overview of the connectors supported by Cel.ai and instructions on how to create custom connectors.</p>"},{"location":"connectors/#what-are-connectors","title":"What Are Connectors?","text":"<p>Cel.ai is a Python framework designed to accelerate the development of omnichannel virtual assistants.  Connectors in Cel.ai are responsible for translating back and forth between the platform-specific message format and Cel.ai's agnostic message format. So connectors are a decpoupling layer between the messaging platform and Cel.ai's message format.</p> <p>Each connector is responsible for handling the specifics of a particular messaging platform, and register in Message Gateway the required endpoints to receive messages from the platform.</p>"},{"location":"connectors/#webhook-overview","title":"Webhook Overview","text":"<p>Message Gateway is the core component of Cel.ai that handles the communication between the assistant and the connectors. It is responsible for processing incoming messages, invoking the assistant, and sending responses back to the connectors.</p> <p>Message Gateway runs a single FastAPI server that listens for incoming messages for all registered connectors. When a connector registers into the gateway, it provides the routes that the gateway should listen to for incoming messages.</p> <p> </p> <p>So in order to receive messages from a messaging platform, you need a public URL that the platform can send messages to. This is where a webhook comes into play. Usually messaging platforms require a public HTTPS endpoint to send messages to your assistant.</p> <p>You can use tools like ngrok to create a secure tunnel to your local server, providing a public URL that can be used to receive webhooks and other HTTP requests from external services. Take a look at the Webhook URL with ngrok guide to learn how to set up ngrok and expose your local server to the internet securely.</p> <p>Some users have reported that they have been able to use pinggy.io to create a public URL for their local server. You can try it out and see if it works for you.</p> Whatsapp and ngrok <p>Whatsapp may not work with ngrok free tier. Today 24 June 2024 only works with ngrok paid tier.</p>"},{"location":"connectors/#supported-connectors","title":"Supported Connectors","text":"<p>Cel.ai comes with out-of-the-box support for the following connectors:</p> <ul> <li>WhatsApp</li> <li>Telegram</li> <li>VAPI.com</li> <li>CLI</li> </ul>"},{"location":"connectors/stream_mode/","title":"Stream Modes","text":"<p>Cel.ai supports different stream modes to define how the client, through the connector, receives responses from the AI assistant. These modes are designed to optimize the user experience based on the specific requirements of the platform and the nature of the interaction. Below are the available stream modes:</p> <p> </p>"},{"location":"connectors/stream_mode/#stream-modes_1","title":"Stream Modes","text":""},{"location":"connectors/stream_mode/#1-direct","title":"1. DIRECT","text":"<p>In the DIRECT mode, each chunk of text generated by the LLM (Large Language Model) is immediately sent to the connector. This mode is ideal for scenarios where real-time response generation is crucial, such as in Text-to-Speech (TTS) applications.</p> <ul> <li>Use Case: Real-time TTS generation.</li> <li>Behavior: Sends each chunk of text as soon as it is generated by the LLM.</li> <li>Advantages: Immediate feedback, suitable for real-time applications.</li> </ul>"},{"location":"connectors/stream_mode/#2-sentence","title":"2. SENTENCE","text":"<p>The SENTENCE mode allows the construction of paragraphs or sentences before sending them to the connector. This mode is particularly useful for messaging platforms where responses can be lengthy. It enables the user to start receiving parts of the response without waiting for the entire response to be generated by the LLM, thus improving response times and user experience.</p> <ul> <li>Use Case: Messaging platforms with potentially lengthy responses.</li> <li>Behavior: Sends paragraphs or sentences as they are constructed.</li> <li>Advantages: Improved response times, better user experience for lengthy messages.</li> </ul>"},{"location":"connectors/stream_mode/#3-full","title":"3. FULL","text":"<p>In the FULL mode, the response is sent to the connector only after the LLM has completed the entire inference process. This mode is ideal for use with Message Enhancers, where the complete response needs to be processed or enhanced before being delivered to the user.</p> <ul> <li>Use Case: Scenarios requiring complete response processing, such as with Message Enhancers.</li> <li>Behavior: Sends the complete response after the LLM has finished generating it.</li> <li>Advantages: Ensures the response is fully processed or enhanced before delivery.</li> </ul>"},{"location":"connectors/stream_mode/#summary","title":"Summary","text":"Stream Mode Use Case Behavior Advantages DIRECT Real-time TTS generation Sends each chunk of text immediately Immediate feedback, real-time applications SENTENCE Messaging platforms with lengthy responses Sends paragraphs or sentences as they are constructed Improved response times, better UX for lengthy messages FULL Scenarios requiring complete response processing Sends the complete response after full generation Ensures fully processed or enhanced responses <p>By selecting the appropriate stream mode, developers can tailor the behavior of their AI assistants to best fit the needs of their specific application and platform, ensuring optimal performance and user satisfaction.</p>"},{"location":"connectors/stream_mode/#setting-the-stream-mode","title":"Setting the Stream Mode","text":"<p>When creating a connector, you can specify the stream mode to define how the responses from the AI assistant are sent to the client. Messaging Platforms like Telegram, WhatsApp usually supports <code>FULL</code> and <code>SENTENCE</code> stream modes. Other connectors such as VAPI require the <code>DIRECT</code> stream mode to support real-time TTS generation.</p> <p>Here is an example of how to set the stream mode when creating a Telegram connector:</p> <pre><code># For this example, we will use the Telegram Connector\nconn = TelegramConnector(\n    token=os.environ.get(\"TELEGRAM_TOKEN\"), \n    stream_mode=StreamMode.FULL\n)\n# Register the connector with the gateway\ngateway.register_connector(conn)\n</code></pre> Smart Message Enhancer <p>When you need to use the <code>SmartMessageEnhancerOpenAI</code> be sure to use the <code>FULL</code> stream mode. This is because the Smart Message Enhancer requires the full response to be processed and enhanced before sending it back to the user.</p>"},{"location":"connectors/webhook_url/","title":"Webhook URL with ngrok","text":""},{"location":"connectors/webhook_url/#introduction","title":"Introduction","text":"<p>Ngrok is a tool that allows you to expose a local server to the internet securely. It creates a secure tunnel to your localhost, providing a public URL that can be used to receive webhooks and other HTTP requests from external services. This is particularly useful for developing and testing integrations with platforms like WhatsApp and Telegram, which require a publicly accessible HTTPS endpoint to send messages and updates.</p>"},{"location":"connectors/webhook_url/#why-use-ngrok","title":"Why Use ngrok?","text":"<ul> <li>Secure Tunneling: Ngrok provides a secure tunnel to your local server, ensuring that your data is encrypted.</li> <li>Public URL: It generates a public URL that can be accessed from anywhere, making it easy to test webhooks and other integrations.</li> <li>Ease of Use: Setting up ngrok is straightforward and requires minimal configuration.</li> </ul>"},{"location":"connectors/webhook_url/#steps-to-set-up-ngrok","title":"Steps to Set Up ngrok","text":""},{"location":"connectors/webhook_url/#step-1-download-and-install-ngrok","title":"Step 1: Download and Install ngrok","text":"<p>Visit the ngrok website and download the appropriate version for your operating system.</p>"},{"location":"connectors/webhook_url/#step-2-sign-up-and-authenticate","title":"Step 2: Sign Up and Authenticate","text":"<ol> <li>Sign up for a free account on the ngrok website.</li> <li>After signing up, you will receive an authentication token.</li> <li> <p>Open a terminal and run the following command to authenticate ngrok with your account:</p> <pre><code>ngrok authtoken YOUR_AUTH_TOKEN\n</code></pre> <p>Replace <code>YOUR_AUTH_TOKEN</code> with the token you received.</p> </li> </ol>"},{"location":"connectors/webhook_url/#step-3-start-ngrok","title":"Step 3: Start ngrok","text":"<ol> <li>Open a terminal.</li> <li> <p>Run the following command to start ngrok and create a tunnel to your local server:</p> <pre><code>ngrok http 5004\n</code></pre> </li> <li> <p>You will see output similar to the following:</p> <pre><code>ngrok by @inconshreveable                                                                                                      (Ctrl+C to quit)\n\nSession Status                online\nAccount                       Your Name (Plan: Free)\nVersion                       2.3.35\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    http://abcd1234.ngrok.io -&gt; http://127.0.0.1:5004\nForwarding                    https://abcd1234.ngrok.io -&gt; http://127.0.0.1:5004\n\nConnections                   ttl     opn     rt1     rt5     p50     p90\n                              0       0       0.00    0.00    0.00    0.00\n</code></pre> </li> <li> <p>Note the <code>https://abcd1234.ngrok.io</code> URL. This is the public HTTPS URL that forwards requests to your local server running on <code>127.0.0.1:5004</code>.</p> </li> </ol>"},{"location":"connectors/webhook_url/#step-4-use-the-public-url","title":"Step 4: Use the Public URL","text":"<ul> <li>Use the <code>https://abcd1234.ngrok.io/whatsapp</code> URL to configure webhooks or other integrations with platforms like WhatsApp.</li> <li>Use <code>https://abcd1234.ngrok.io/telegram</code> for Telegram integrations.</li> <li>Ensure that your Cel.ai Assistant is running on local server <code>127.0.0.1:5004</code>.</li> </ul>"},{"location":"connectors/webhook_url/#conclusion","title":"Conclusion","text":"<p>By following these steps, you have successfully set up ngrok to create a secure tunnel to your local server. You now have a public HTTPS URL that can be used to receive messages and updates from platforms like WhatsApp and Telegram. This setup is ideal for development and testing purposes, allowing you to work with webhooks and other integrations seamlessly.</p>"},{"location":"connectors/whatsapp/","title":"WhatsApp Connector","text":"<p>The <code>Whatsapp Connector</code> is a component of the Cel.ai framework that allows seamless integration with the WhatsApp Cloud API. This connector translates messages between WhatsApp and Cel.ai's agnostic message format, enabling the development of virtual assistants that can interact with users on WhatsApp.</p>"},{"location":"connectors/whatsapp/#initialization","title":"Initialization","text":"<p>To initialize the <code>Whatsapp Connector</code>, you need to provide the Meta Access Token, Phone Number ID, and a verification token. Optionally, you can also specify an endpoint prefix and stream mode.</p>"},{"location":"connectors/whatsapp/#parameters","title":"Parameters","text":"<ul> <li><code>token</code> (str): The Meta Access Token. This is required.</li> <li><code>phone_number_id</code> (str): The Meta Phone Number ID. This is required.</li> <li><code>verify_token</code> (str): The verification token used for webhook verification. This is required.</li> <li><code>endpoint_prefix</code> (str, optional): The prefix for the webhook endpoint. Defaults to <code>\"/whatsapp\"</code>.</li> <li><code>stream_mode</code> (StreamMode, optional): The mode for streaming messages. Defaults to <code>StreamMode.SENTENCE</code>. See Stream Modes for more information.</li> </ul>"},{"location":"connectors/whatsapp/#example","title":"Example","text":"<pre><code>from cel.connectors.whatsapp.whatsapp_connector import WhatsappConnector\n\nwsp = WhatsappConnector(token=os.getenv(\"WHATSAPP_TOKEN\"), \n                    phone_number_id=os.getenv(\"WHATSAPP_PHONE_NUMBER_ID\"),\n                    verify_token=\"123456\",\n                    endpoint_prefix=\"/whatsapp\",\n                    stream_mode=StreamMode.FULL)\n</code></pre>"},{"location":"connectors/whatsapp/#attributes","title":"Attributes","text":"<ul> <li><code>token</code> (str): The Meta Access Token.</li> <li><code>phone_number_id</code> (str): The Meta Phone Number ID.</li> <li><code>base_url</code> (str): The base URL for the WhatsApp Cloud API.</li> <li><code>url</code> (str): The full URL for sending messages.</li> <li><code>verify_token</code> (str): The verification token. Setup this token in the WhatsApp Cloud API settings.</li> <li><code>endpoint_prefix</code> (str): The prefix for the webhook endpoint.</li> <li><code>stream_mode</code> (StreamMode): The mode for streaming messages.</li> <li><code>verification_handler</code> (callable): The handler for verification requests.</li> </ul>"},{"location":"connectors/whatsapp/#whatsapp-assistant","title":"Whatsapp Assistant","text":"main.py<pre><code># Import Cel.ai modules\nfrom cel.gateway.message_gateway import MessageGateway, StreamMode\nfrom cel.message_enhancers.smart_message_enhancer_openai import SmartMessageEnhancerOpenAI\nfrom cel.assistants.macaw.macaw_assistant import MacawAssistant\nfrom cel.prompt.prompt_template import PromptTemplate\nfrom cel.connectors.whatsapp.whatsapp_connector import WhatsappConnector\n\n\n# Setup prompt\nprompt = \"\"\"You are an AI assistant. Called Celia. You can help a user to buy Bitcoins.\"\"\"\nprompt_template = PromptTemplate(prompt)\n\n# Create the assistant based on the Macaw Assistant \nast = MacawAssistant(\n    prompt=prompt_template\n)\n\n# Create the Message Gateway - This component is the core of the assistant\n# It handles the communication between the assistant and the connectors\ngateway = MessageGateway(\n    webhook_url=os.environ.get(\"WEBHOOK_URL\"),\n    assistant=ast,\n    host=\"127.0.0.1\", port=5004,\n    message_enhancer=SmartMessageEnhancerOpenAI(),\n    delivery_rate_control=False\n)\n\nwsp = WhatsappConnector(token=os.getenv(\"WHATSAPP_TOKEN\"), \n                    phone_number_id=os.getenv(\"WHATSAPP_PHONE_NUMBER_ID\"),\n                    verify_token=\"123456\",\n                    stream_mode=StreamMode.FULL)\n\n# Register the connector with the gateway\ngateway.register_connector(wsp)\n\n# Then start the gateway and begin processing messages\ngateway.run()\n</code></pre>"},{"location":"connectors/whatsapp/#notes","title":"Notes","text":"<ul> <li>Ensure that the <code>WHATSAPP_TOKEN</code> Meta Access Token and <code>WHATSAPP_PHONE_NUMBER_ID</code> Phone Number ID are valid and have the necessary permissions to interact with the WhatsApp Cloud API.</li> <li>Get the <code>WHATSAPP_TOKEN</code> and <code>WHATSAPP_PHONE_NUMBER_ID</code> from the WhatsApp Business API settings.</li> <li>The verification token is used to verify the webhook endpoint with WhatsApp. Make sure it matches the token you set in the WhatsApp Cloud API settings.</li> </ul> <p>This documentation provides an overview of the <code>WhatsappConnector</code> and how to initialize and use it within the Cel.ai framework. For more detailed information, refer to the Cel.ai official documentation.</p>"},{"location":"middlewares/","title":"Index","text":""},{"location":"middlewares/#middlewares","title":"Middlewares","text":"<p>Cel.ai supports the use of middlewares to extend and customize the functionality of your virtual assistants. This allows for greater flexibility and control over how your assistant processes and responds to user interactions. Middlewares can be used to add custom logic, perform data validation, or perform authentication, rate limiting, blacklisting, and more.</p> <p>You will find useful middlewares for decoding messages into a suitable format for Natural Language Processing (NLP) models. Take a look of <code>GeodecodingMiddleware</code> which decodes a message shared location into a human-readable address using the Google Maps API. </p> <p>So you can easily create your own middleware to add custom functionality to your assistant. For example, you can create a <code>PDFDecodingMiddleware</code> to decode a PDF file shared by the user then convert it into a text format that can be processed by the assistant or sent to a Embedding model for further processing.</p>"},{"location":"middlewares/#message-format","title":"Message Format","text":"<p>The message input to the middleware is a internal message format that is agnostic to the platform. So you can code your middleware and use it across different platforms without any changes. You can share this middleware with the community and use it in different projects.</p>"},{"location":"middlewares/#available-middlewares","title":"Available Middlewares","text":""},{"location":"middlewares/#geodecoding-middleware","title":"Geodecoding Middleware","text":"<p>Decodes a message shared location into a human-readable address using the Google Maps API. For example, if a user shares a location in Telegram, the middleware will decode the location into a human-readable address. If you add this middleware to your assistant, you can easily orientate the inference model to the user's location. Some times high level references such as neighborhoods, cities, or monuments can be useful to the assistant to provide better responses.</p>"},{"location":"middlewares/#invitationguard-middleware","title":"InvitationGuard Middleware","text":"<p>Useful for creating a invitation system for your assistant. If you need to restrict access to your assistant, you can use this middleware to create an invitation system. It handles invitation codes in messages and allows users to claim an invitation. You can also create, revoke, and get invitations. </p>"},{"location":"middlewares/#features","title":"Features:","text":"<ul> <li>Create, revoke and get invitations</li> <li>Claim an invitation</li> <li>Handle invitation codes in messages</li> <li>Backdoor code to bypass invitation system</li> </ul>"},{"location":"middlewares/#triggered-events","title":"Triggered events:","text":"<ul> <li><code>new_conversation</code>: Event called when a new conversation is started.</li> <li><code>rejected_code</code>: Event called when a user enters an invalid code.</li> <li><code>admin_login</code>: Event called when a user logs in as admin.</li> <li><code>admin_logout</code>: Event called when a user logs out as admin.</li> </ul>"},{"location":"middlewares/#deepgramstt-middleware","title":"DeepgramSTT Middleware","text":"<p>Deepgram Speech to Text Middleware for voice messages. It wll set the <code>message.text</code> with the STT result. This function is a wrapper for the DeepgramAdapter.  You can change the voice ID, language, and other parameters in the <code>DeepgramAdapter</code> class. With this middleware, you can easily add incoming voice message support to your assistant.</p>"},{"location":"middlewares/#blacklist-middleware","title":"Blacklist Middleware","text":"<p>The Blacklist Middleware allows you to block users from interacting with your assistant. You can add users to the blacklist and prevent them from sending messages to your assistant. This can be useful for handling abusive users or for restricting access to your assistant.</p> <p>There are two types of blacklists middleware: - In-memory blacklist: The blacklist is stored in memory and is not persistent. This means that the blacklist will be cleared when the assistant is restarted. <code>InMemBlackListMiddleware</code></p> <ul> <li>Redis blacklist: The blacklist is stored in a Redis database and is persistent. This means that the blacklist will be saved even if the assistant is restarted <code>RedisBlackListAsyncMiddleware</code></li> </ul>"},{"location":"prompt/","title":"PromptTemplate Class Documentation","text":"<p>The <code>PromptTemplate</code> class provides a mechanism to dynamically compile a string template with placeholders, using state data that can be static values or callable functions, including asynchronous functions. This is particularly useful in applications like chatbots or interactive prompt systems where template customization is required based on dynamic data sources or processing.</p>"},{"location":"prompt/#class-definition","title":"Class Definition","text":"<pre><code>class PromptTemplate:\n    def __init__(self, prompt: str)\n</code></pre>"},{"location":"prompt/#methods","title":"Methods","text":""},{"location":"prompt/#__init__self-prompt-str","title":"<code>__init__(self, prompt: str)</code>","text":"<p>Initializes a new instance of the <code>PromptTemplate</code> with a given prompt that contains placeholders.</p> <ul> <li>Parameters:</li> <li><code>prompt</code> (str): A string containing placeholders in the format <code>{key}</code>.</li> </ul>"},{"location":"prompt/#async-compileself-state-dict-lead-conversationlead-message-str-str","title":"<code>async compile(self, state: Dict, lead: ConversationLead, message: str) -&gt; str</code>","text":"<p>Compiles the prompt with the supplied state data, resolving each placeholder with the respective value from the state. Values in the state dictionary can be static or callable. Callable objects can also be asynchronous functions.</p> <ul> <li>Parameters:</li> <li><code>state</code> (Dict): A dictionary where keys are placeholder names and values are the data or callable that returns data.</li> <li><code>lead</code> (ConversationLead): An instance of <code>ConversationLead</code> used for accessing conversation-specific data.</li> <li> <p><code>message</code> (str): The message context used in callable functions when compiling the template.</p> </li> <li> <p>Returns:</p> </li> <li> <p><code>str</code>: The compiled string with all placeholders replaced by their corresponding values.</p> </li> <li> <p>Example:</p> </li> </ul> <pre><code>    async def get_contacts_async(lead: ConversationLead):\n        return [\"Juan\", \"Pedro\", \"Maria\"]\n\n    def get_balance(lead: ConversationLead, message: str):\n        return {\n            \"checking\": 1000,\n            \"savings\": 5000\n        }\n\n    prompt = \"\"\"Hola, {name}. Tienes {messages} mensajes nuevos.\n    Tiene los siguientes contactos: {contacts}.\n    Su saldo es: \n    {balance}\"\"\"\n\n    state = {\n        \"name\": \"Juan\",\n        \"messages\": lambda: 5,\n        \"contacts\": get_contacts_async,\n        \"balance\": get_balance\n    }\n\n    template = PromptTemplate(prompt)\n    result = await template.compile(state, lead, \"Hola\")\n  ```\n\n#### `async call_function(self, func: Callable, message: Optional[str] = None, current_state: Optional[dict] = None, lead: Optional[ConversationLead] = None) -&gt; str`\n\nExecutes the given function with dynamic arguments based on the provided context, which includes the lead, message, and state. Handles both synchronous and asynchronous functions. \n\n- **Parameters:**\n  - `func` (Callable): The function to be invoked with specific arguments.\n  - `message` (str, optional): Contextual message to pass as an argument, if applicable.\n  - `current_state` (dict, optional): Current state that might be needed inside the function.\n  - `lead` (ConversationLead, optional): Required context containing the lead data required to call the function.\n\n- **Returns:**\n  - `str`: Result from the function call as a JSON string if it's a dictionary, or a simple string.\n\n- **Example:**\n\n```python\n  async def custom_function(lead: ConversationLead, session_id: str):\n      return {\"data\": \"Result\"}\n\n  response = await template.call_function(custom_function, lead=lead)\n</code></pre>"},{"location":"prompt/#considerations","title":"Considerations","text":"<ul> <li>Ensure that any callable within the <code>state</code> dictionary correctly handles the parameters it expects.</li> <li>Use <code>asyncio</code> for handling asynchronous operations efficiently.</li> <li>Handle exceptions within the <code>call_function</code> to avoid abrupt failures.</li> </ul> <p>This class can be extended and modified to handle more complex scenarios and additional data processing requirements. It forms a core component in scenarios demanding template-based dynamic content generation. </p>"},{"location":"state/state_manager/","title":"State Manager","text":"<p>The State Manager in Cel.ai provides a simple and efficient way to store and retrieve state information during a conversation. This feature is crucial for maintaining context and ensuring a seamless user experience.</p>"},{"location":"state/state_manager/#what-is-the-state-for","title":"What is the State for?","text":"<p>The state in a conversation refers to the information that is relevant to the current context of the conversation. For example, if the user asks a question and the assistant needs to look up information to provide an answer, the state can be used to store the question and the information retrieved so that the assistant can refer back to it later in the conversation.</p> <p>For example, if the user asks for how much money they have in their account, the assistant can store the user's account balance in the state, then Cel.ai can inject that information into the prompt before processing the inference.</p> <p><code>PromptTemplate</code> can reference any key-value pair stored in the state. For example, if the state contains the key <code>account_balance</code>, the <code>PromptTemplate</code> can reference it as <code>{{account_balance}}</code>.</p>"},{"location":"state/state_manager/#advantages","title":"Advantages","text":"<ul> <li>Automatic State Management: The state is automatically saved and committed when using the <code>async with</code> block.</li> <li>Error Handling: If an error occurs within the <code>async with</code> block, the state changes are rolled back, ensuring data integrity.</li> <li>Simplicity: The state is managed as a simple key-value store, making it easy to use and understand.</li> <li>Data Persistence: The state is persisted across multiple messages and sessions, allowing for continuity in the conversation.</li> </ul>"},{"location":"state/state_manager/#usage","title":"Usage","text":"<p>To use the State Manager, you need to access it via the <code>RequestContext</code> or <code>FunctionContext</code> object. Here is an example of how to use it within an event handler for a message event:</p> <pre><code>from cel.assistants.request_context import RequestContext\n\n@ast.event('message')\nasync def handle_message(session, ctx: RequestContext):\n    log.debug(f\"Got message event: {ctx.message}\")\n\n    # Using the state manager within an async with block\n    async with ctx.state_manager() as state:\n        count = state.get(\"count\", 0)\n        count += 1\n        state[\"count\"] = count\n\n    # Send a response\n    await ctx.connector.send_text_message(ctx.lead, f\"Message count: {count}\")\n    return ctx.cancel_ai_response()\n</code></pre>"},{"location":"state/state_manager/#example","title":"Example","text":"<p>Here is a complete example demonstrating how to use the State Manager to count the number of messages received from a user:</p> <pre><code>import asyncio\nimport os\nfrom loguru import logger as log\nfrom dotenv import load_dotenv\nfrom cel.connectors.telegram import TelegramConnector\nfrom cel.gateway.message_gateway import MessageGateway, StreamMode\nfrom cel.message_enhancers.smart_message_enhancer_openai import SmartMessageEnhancerOpenAI\nfrom cel.assistants.macaw.macaw_assistant import MacawAssistant\nfrom cel.prompt.prompt_template import PromptTemplate\nfrom cel.assistants.request_context import RequestContext\n\n# Load environment variables\nload_dotenv()\n\n# Setup prompt\nprompt = \"You are an assistant called Celia. Keep responses short and to the point. Don't use markdown formatting in your responses.\"\nprompt_template = PromptTemplate(prompt)\n\n# Create the assistant\nast = MacawAssistant(prompt=prompt_template)\n\n# Event handler for the message event\n@ast.event('message')\nasync def handle_message(session, ctx: RequestContext):\n    log.debug(f\"Got message event: {ctx.message}\")\n\n    # Using the state manager within an async with block\n    async with ctx.state_manager() as state:\n        count = state.get(\"count\", 0)\n        count += 1\n        state[\"count\"] = count\n\n    # Send a response\n    await ctx.connector.send_text_message(ctx.lead, f\"Message count: {count}\")\n    return ctx.cancel_ai_response()\n\n# Create the Message Gateway\ngateway = MessageGateway(\n    assistant=ast,\n    host=\"127.0.0.1\", port=5004,\n    message_enhancer=SmartMessageEnhancerOpenAI()\n)\n\n# Use the Telegram connector\nconn = TelegramConnector(\n    token=os.environ.get(\"TELEGRAM_TOKEN\n\n\"),\n\n\n    stream_mode=StreamMode.FULL\n)\ngateway.register_connector(conn)\n\n# Start the gateway\ngateway.run(enable_ngrok=True)\n</code></pre>"},{"location":"state/state_manager/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>async with</code> Block: Always use the <code>async with</code> block for state management to ensure automatic saving and error handling.</li> <li>Avoid Manual State Saving: While you can manually save the state using <code>await state.save_state()</code>, it is recommended to rely on the <code>async with</code> block for simplicity and reliability.</li> <li>Handle Errors Gracefully: If an error occurs within the <code>async with</code> block, the state changes will be rolled back, ensuring data integrity.</li> </ul> <p>By following these best practices, you can effectively manage state in your Cel.ai assistant, providing a robust and seamless user experience.</p> <p>alejamp: crea un archivo .md con eso</p> <p>GitHub Copilot: ### Plan</p> <ol> <li>Create a new Markdown file named <code>assistant.md</code>.</li> <li>Copy the provided Python code and comments into the Markdown file.</li> <li>Format the content appropriately using Markdown syntax.</li> </ol>"},{"location":"state/state_manager/#code","title":"Code","text":"<p><pre><code># State Management with Cel.ai\n\nThis is a simple example of an AI Assistant implemented using the Cel.ai framework. It serves as a basic demonstration of how to get started with Cel.ai for creating intelligent assistants.\n\n**Framework**: Cel.ai  \n**License**: MIT License\n\nThis script is part of the Cel.ai example series and is intended for educational purposes.\n\n## Usage\n\nConfigure the required environment variables in a `.env` file in the root directory of the project. The required environment variables are:\n- `WEBHOOK_URL`: The webhook URL for the assistant, you can use ngrok to create a public URL for your local server.\n\n\n-\n\n `TELEGRAM_TOKEN`: The Telegram bot token for the assistant. You can get this from the BotFather on Telegram.\n\nThen run this script to see a basic AI assistant in action.\n\n**Note**: Please ensure you have the Cel.ai framework installed in your Python environment prior to running this script.\n\n## Code\n\n```python\n# LOAD ENV VARIABLES\nimport asyncio\nimport os\nfrom loguru import logger as log\n# Load .env variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# REMOVE THIS BLOCK IF YOU ARE USING THIS SCRIPT AS A TEMPLATE\n# -------------------------------------------------------------\nimport sys\nfrom pathlib import Path\n# Add parent directory to path\npath = Path(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(str(path.parents[1]))\n# -------------------------------------------------------------\n\n# Import Cel.ai modules\nfrom cel.connectors.telegram import TelegramConnector\nfrom cel.gateway.message_gateway import MessageGateway, StreamMode\nfrom cel.message_enhancers.smart_message_enhancer_openai import SmartMessageEnhancerOpenAI\nfrom cel.assistants.macaw.macaw_assistant import MacawAssistant\nfrom cel.prompt.prompt_template import PromptTemplate\nfrom cel.assistants.request_context import RequestContext\n\n# Setup prompt\nprompt = \"You are an assistant called Celia. Keep responses short and to the point.\\\nDon't use markdown formatting in your responses.\"\n\nprompt_template = PromptTemplate(prompt)\n\n# Create the assistant based on the Macaw Assistant \n# NOTE: Make sure to provide api key in the environment variable `OPENAI_API_KEY`\n# add this line to your .env file: OPENAI_API_KEY=your-key\n# or uncomment the next line and replace `your-key` with your OpenAI API key\n# os.environ[\"OPENAI_API_KEY\"] = \"your-key..\"\nast = MacawAssistant(prompt=prompt_template)\n\n# Event handler for the message event\n# This example demonstrates how to use the state manager to store and retrieve state\n# Count the number of messages received from the user and send a response with the count\n# ---------------------------------------------------------------------------\n@ast.event('message')\nasync def handle_message(session, ctx: RequestContext):\n    log.debug(f\"Got message event: {ctx.message}\")\n\n    # State Manager: Celai provides a simple way to store and retrieve state\n    # for the conversation. The state is accessible via ctx.state_manager()\n    # For example: ctx.state_manager() returns an AsyncStateManager object\n    # which can be used to store and retrieve state for the conversation.\n    async with ctx.state_manager() as state:\n        count = state.get(\"count\", 0)\n        count += 1\n        state[\"count\"] = count\n        # Add more logic and changes to the state here\n        # For example: state[\"key\"] = value\n        # The state is a simple key-value store that can be used to store\n        # any data related to the conversation.\n\n    # Once you leave the async with block, the state will be automatically saved\n    # and committed to the state store.\n\n    # You can still access the state outside the async with block\n    # but you won't be able to save changes to the state automatically\n    # If you need to save changes to the state outside the async with block\n    # you can call: await state.save_state()\n\n    # We strongly recommend using the async with block for state management\n\n    # If a error occurs inside the async with block, the state will not be saved\n    # and the changes will be rolled back.So you can safely use the async with block\n\n    # Send a response\n    await ctx.connector.send_text_message(ctx.lead, f\"Message count: {count}\")\n    return ctx.cancel_ai_response()\n# ---------------------------------------------------------------------------\n\n# Create the Message Gateway - This component is the core of the assistant\n# It handles the communication between the assistant and the connectors\ngateway = MessageGateway(\n    assistant=ast,\n    host=\"127.0.0.1\", port=5004,\n    message_enhancer=SmartMessageEnhancerOpenAI()\n)\n\n# For this example, we will use the Telegram connector\nconn = TelegramConnector(\n    token=os.environ.get(\"TELEGRAM_TOKEN\"), \n    stream_mode=StreamMode.FULL\n)\n# Register the connector with the gateway\ngateway.register_connector(conn)\n\n# Then start the gateway and begin processing messages\n# gateway.run()\ngateway.run(enable_ngrok=True)\n</code></pre> ```</p> <p>Save this content into a file named <code>assistant.md</code>.</p>"}]}